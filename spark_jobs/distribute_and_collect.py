from pyspark import SparkContext
import requests
import pandas as pd

# Edge worker åˆ—è¡¨
EDGE_NODES = [
    {"name": "edge1", "url": "http://192.168.1.11:8000/read_data"},
    {"name": "edge2", "url": "http://192.168.1.12:8000/read_data"},
    {"name": "edge3", "url": "http://192.168.1.13:8000/read_data"},
]

sc = SparkContext("spark://192.168.1.10:7077", "EdgeDataCollector")

def fetch_from_edge(edge):
    try:
        res = requests.get(edge["url"], timeout=10)
        js = res.json()
        if "rows" not in js:
            return []
        return js["rows"]
    except Exception as e:
        print(f"âš ï¸ {edge['name']} error: {e}")
        return []

# å¹¶è¡Œå‘æ‰€æœ‰ Edge è¯·æ±‚æ•°æ®
rdd = sc.parallelize(EDGE_NODES)
all_data = rdd.flatMap(fetch_from_edge).collect()

sc.stop()

# æ±‡æ€»æˆ DataFrame
df = pd.DataFrame(all_data)
print(f"âœ… Received {len(df)} rows from {len(EDGE_NODES)} edge nodes")

# ä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶ä¾›åç»­åˆ†æ
df.to_csv("data/combined/edge_merged.csv", index=False)

# ç®€å•åˆ†æï¼šå¹³å‡æ¸©åº¦ã€æ¹¿åº¦
if not df.empty:
    summary = df.agg({
        "temperature": ["mean", "min", "max"],
        "humidity": ["mean", "min", "max"]
    })
    print("ğŸ“Š Summary statistics:\n", summary)
