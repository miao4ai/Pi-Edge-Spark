# ğŸ§  Pi-Edge-Spark  
**Pi-Edge-Spark** is a distributed real-time ETL and streaming analytics platform  
built for edge clusters such as Raspberry Pi networks.  

It combines **PySpark Structured Streaming**, **Redis Streams**, and **MinIO**  
to provide an online data-processing framework where each edge node continuously  
produces sensor or event data, and a central Spark cluster performs live analytics,  
aggregation, and anomaly detection.

---

## âœ¨ Features

- **Dynamic edge discovery** â€” supports variable number of worker nodes.
- **Secure architecture** â€” no IPs hard-coded; cluster info injected at runtime.
- **Streaming ETL** â€” continuous ingestion + online aggregation.
- **Edge-to-Cloud bridge** â€” lightweight message broker (Redis Streams / Redpanda).
- **Hybrid storage** â€” MinIO (S3-compatible) for history, TimescaleDB for metrics.
- **Airflow orchestration** â€” DAGs trigger ETL & upload jobs automatically.
- **Visual analytics** â€” optional Grafana or Streamlit dashboards.

---

## ğŸ§± Architecture Overview
```text
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚               Airflow DAGs                   â”‚
    â”‚  â€¢ Schedule ETL and upload jobs              â”‚
    â”‚  â€¢ Monitor cluster health                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚        Spark Structured Streaming Cluster     â”‚
    â”‚  â€¢ Subscribe to Redis / Kafka topics          â”‚
    â”‚  â€¢ Aggregate, clean, and detect anomalies     â”‚
    â”‚  â€¢ Output â†’ MinIO / TimescaleDB               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–²
                       â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Edge Node 1  â”‚ Edge Node 2  â”‚ Edge Node N  â”‚
    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
    â”‚ â€¢ Sensor dataâ”‚ â€¢ File tail  â”‚ â€¢ MQTT input  â”‚
    â”‚ â€¢ Python pub â”‚ â€¢ Redis pub  â”‚ â€¢ local ETL   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
             Message Broker Layer
      (Redis Streams / Kafka / Redpanda)
```

---

## âš™ï¸ Core Components

| Layer | Component | Description |
|-------|------------|-------------|
| **Edge** | ğŸ **Python Stream Producer** | Continuously reads local sensors or logs and publishes structured JSON messages to Redis Streams or Redpanda. |
| **Broker** | ğŸ§© **Redis Streams / Redpanda** | Acts as the lightweight message queue between Edge nodes and the central Spark cluster. |
| **Compute** | ğŸ”¥ **Spark Structured Streaming** | Performs real-time aggregation, cleaning, and anomaly detection across all Edge nodes. |
| **Storage** | â˜ï¸ **MinIO / TimescaleDB** | Stores processed results, historical archives, and time-series metrics. |
| **Orchestration** | âš™ï¸ **Airflow** | Automates scheduled ETL runs, uploads, and cluster monitoring workflows. |
| **Visualization** | ğŸ“Š **Grafana / Streamlit** | Provides real-time dashboards and system health visualization for devices and KPIs. |

---

## ğŸ“‚ Project File Structure

```text
Pi-Edge-Spark/
â”œâ”€â”€ conf/
â”‚   â”œâ”€â”€ cluster.yaml              # Cluster configuration (auto or manual worker list)
â”‚   â””â”€â”€ spark-env.sh              # Auto-generated Spark master environment file
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                      # Edge-sourced raw CSV or JSON input data
â”‚   â””â”€â”€ processed/                # Processed & aggregated data outputs
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ pi_edge_streaming_dag.py  # Airflow DAG for ETL orchestration and upload
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ init_cluster_env.py       # Detects master IP, writes spark-env.sh
â”‚   â”œâ”€â”€ edge_producer.py          # Example edge node streaming data producer
â”‚   â”œâ”€â”€ upload_to_minio.py        # Uploads processed results to MinIO
â”‚   â”œâ”€â”€ setup_minio.py            # Initializes MinIO bucket and access policy
â”‚   â””â”€â”€ run_local_test.sh         # Local cluster test runner script
â”‚
â”œâ”€â”€ spark_jobs/
â”‚   â”œâ”€â”€ streaming_etl.py          # Core Spark Structured Streaming job
â”‚   â””â”€â”€ batch_etl.py              # Offline fallback ETL process
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yml        # Optional: Spark + Redis + MinIO stack
â”‚   â””â”€â”€ airflow.dockerfile        # Lightweight Airflow image (for Pi/ARM)
â”‚
â”œâ”€â”€ requirements.txt              # Python dependency list
â””â”€â”€ README.md                     # Project documentation
```