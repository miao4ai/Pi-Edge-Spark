# ğŸ§  Pi-Edge-Spark  
**Edge-Orchestrated Data Processing with Apache Spark on Raspberry Pi**

---

## ğŸ“˜ Overview
**Pi-Edge-Spark** demonstrates how to use **Apache Spark** as a central orchestrator to coordinate multiple **edge-computing devices** (such as Raspberry Pi) for distributed **data cleaning, preprocessing, and aggregation**.

Instead of sending all raw data to the cloud, each edge node performs lightweight ETL locally â€” reducing bandwidth and latency â€” while Spark manages job distribution and global analytics.

> ğŸ’¡ Goal: Build a mini **Edge-Cloud Collaborative Data Pipeline** powered by Spark + Python.

## ğŸ—ï¸ System Architecture

```text
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚       Spark Master (Cloud)    â”‚
                  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
                  â”‚ â€¢ Job Orchestrator            â”‚
                  â”‚ â€¢ Global Aggregation          â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚ Spark Jobs (HTTP/MQTT)
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚                   â”‚                   â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Edge Node 1 â”‚    â”‚  Edge Node 2 â”‚    â”‚  Edge Node 3 â”‚
      â”‚ (Raspberry)  â”‚    â”‚ (Raspberry)  â”‚    â”‚ (Raspberry)  â”‚
      â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
      â”‚ â€¢ SparkWorkerâ”‚    â”‚ â€¢ SparkWorkerâ”‚    â”‚ â€¢ SparkWorkerâ”‚
      â”‚ â€¢ Edge Agent â”‚    â”‚ â€¢ Edge Agent â”‚    â”‚ â€¢ Edge Agent â”‚
      â”‚ â€¢ Local ETL  â”‚    â”‚ â€¢ Local ETL  â”‚    â”‚ â€¢ Local ETL  â”‚
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                   â”‚                   â”‚
        Cleaned CSV          Cleaned CSV         Cleaned CSV
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       Aggregated by Spark Master
```

## âš™ï¸ Features
- ğŸ§© **Edge-aware Spark Jobs** â€” Spark Driver sends ETL tasks to Raspberry Pi nodes.  
- ğŸ§® **Distributed Data Cleaning** â€” Each edge node runs its own Python ETL agent.  
- ğŸ“¤ **Unified Aggregation** â€” Cleaned data is sent back to the Spark cluster or shared storage.  
- ğŸŒ **Lightweight Communication** â€” Implemented via REST API (Flask) or MQTT.  
- âš¡ **Low-Cost Deployment** â€” Runs entirely on 4 Raspberry Pi boards.  

---

## ğŸ“‚ Project Structure

```text
pi-edge-spark/
â”œâ”€â”€ edge_jobs/                     # Edge node ETL and preprocessing scripts
â”‚   â”œâ”€â”€ edge_clean_job.py          # Cleans local raw data on Raspberry Pi
â”‚   â””â”€â”€ edge_feature_job.py        # Extracts local statistical features
â”‚
â”œâ”€â”€ spark_jobs/                    # Spark driver orchestration and aggregation
â”‚   â”œâ”€â”€ distribute_task.py         # Sends ETL tasks to edge nodes via HTTP
â”‚   â”œâ”€â”€ aggregate_results.py       # Aggregates cleaned data from all edges
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ edge_comm.py           # Communication helper (REST / MQTT)
â”‚
â”œâ”€â”€ edge_agent/                    # Lightweight Flask agent running on edge
â”‚   â””â”€â”€ edge_server.py             # Listens for Spark task requests and runs jobs
â”‚
â”œâ”€â”€ conf/                          # Spark configuration files
â”‚   â”œâ”€â”€ spark-env.sh               # Environment variables for Spark runtime
â”‚   â””â”€â”€ workers                    # List of edge worker IP addresses
â”‚
â”œâ”€â”€ scripts/                       # Cluster control and pipeline execution
â”‚   â”œâ”€â”€ start_cluster.sh           # Starts Spark master and worker processes
â”‚   â”œâ”€â”€ start_edge_agents.sh       # Starts Flask agents on all Raspberry Pis
â”‚   â”œâ”€â”€ submit_etl_pipeline.sh     # Unified entry point to trigger Spark jobs
â”‚
â””â”€â”€ README.md                      # Project documentation and setup guide
```